#!/usr/bin/env python3
"""
validate_hdf5_data.py
ä½œç”¨ï¼šéªŒè¯HDF5æ•°æ®æ–‡ä»¶ï¼Œå°†å¤šä¸ªæ‘„åƒå¤´è§†è§’åˆå¹¶æˆä¸€ä¸ªè§†é¢‘
"""

import h5py
import cv2
import numpy as np
import argparse
import os
from pathlib import Path
import json


class HDF5VideoValidator:
    def __init__(self, hdf5_path, output_video_path=None):
        self.hdf5_path = Path(hdf5_path)
        self.output_video_path = output_video_path or (self.hdf5_path.parent / "merged_video.mp4")
        
        # è§†é¢‘å‚æ•°
        self.fps = 30.0
        self.fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        
        print(f"ğŸ“ è¯»å–HDF5æ–‡ä»¶: {self.hdf5_path}")
        print(f"ğŸ¬ è¾“å‡ºè§†é¢‘: {self.output_video_path}")
    
    def load_hdf5_data(self):
        """åŠ è½½HDF5æ•°æ®"""
        try:
            with h5py.File(self.hdf5_path, 'r') as f:
                print("\nğŸ“Š HDF5æ–‡ä»¶ç»“æ„:")
                self._print_hdf5_structure(f, indent=0)
                
                # åŠ è½½è§‚æµ‹æ•°æ®ï¼ˆæ‘„åƒå¤´å›¾åƒï¼‰
                observation_data = {}
                if 'observation' in f:
                    obs_group = f['observation']
                    for key in obs_group.keys():
                        if key.startswith('camera'):
                            data = obs_group[key][:]
                            observation_data[key] = data
                            print(f"ğŸ“· åŠ è½½ {key}: {data.shape}")
                
                # åŠ è½½è½¨è¿¹æ•°æ®
                trajectory_data = {}
                if 'trajectory' in f:
                    traj_group = f['trajectory']
                    for key in traj_group.keys():
                        data = traj_group[key][:]
                        trajectory_data[key] = data
                        print(f"ğŸ“ˆ åŠ è½½ trajectory/{key}: {data.shape}")
                
                # åŠ è½½å…ƒæ•°æ®
                metadata = {}
                for attr_name in f.attrs.keys():
                    metadata[attr_name] = f.attrs[attr_name]
                    print(f"â„¹ï¸  {attr_name}: {metadata[attr_name]}")
                
                return observation_data, trajectory_data, metadata
                
        except Exception as e:
            print(f"âŒ åŠ è½½HDF5æ–‡ä»¶å¤±è´¥: {e}")
            return None, None, None
    
    def _print_hdf5_structure(self, group, indent=0):
        """é€’å½’æ‰“å°HDF5ç»“æ„"""
        prefix = "  " * indent
        for key in group.keys():
            item = group[key]
            if isinstance(item, h5py.Group):
                print(f"{prefix}ğŸ“ {key}/")
                self._print_hdf5_structure(item, indent + 1)
            else:
                print(f"{prefix}ğŸ“„ {key}: {item.shape} {item.dtype}")
    
    def create_merged_video(self, observation_data, trajectory_data, metadata):
        """åˆ›å»ºåˆå¹¶è§†é¢‘"""
        if not observation_data:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æ‘„åƒå¤´æ•°æ®")
            return False
        
        # è·å–æ‘„åƒå¤´æ•°æ®
        camera_keys = sorted([k for k in observation_data.keys() if k.startswith('camera')])
        print(f"ğŸ“· æ‰¾åˆ°æ‘„åƒå¤´: {camera_keys}")
        
        if len(camera_keys) == 0:
            print("âŒ æ²¡æœ‰æ‘„åƒå¤´æ•°æ®")
            return False
        
        # è·å–è§†é¢‘å‚æ•°
        first_camera = observation_data[camera_keys[0]]
        num_frames, height, width = first_camera.shape[:3]
        
        # ç¡®å®šåˆå¹¶å¸ƒå±€
        if len(camera_keys) == 1:
            # å•æ‘„åƒå¤´ï¼Œç›´æ¥ä½¿ç”¨åŸå›¾
            merged_width = width
            merged_height = height
            layout = "single"
        elif len(camera_keys) == 2:
            # åŒæ‘„åƒå¤´ï¼Œå·¦å³æ’åˆ—
            merged_width = width * 2
            merged_height = height
            layout = "horizontal"
        elif len(camera_keys) <= 4:
            # å¤šæ‘„åƒå¤´ï¼Œ2x2å¸ƒå±€
            merged_width = width * 2
            merged_height = height * 2
            layout = "grid"
        else:
            # æ›´å¤šæ‘„åƒå¤´ï¼Œåªå–å‰4ä¸ª
            camera_keys = camera_keys[:4]
            merged_width = width * 2
            merged_height = height * 2
            layout = "grid"
        
        print(f"ğŸ¬ è§†é¢‘å‚æ•°: {num_frames}å¸§, {merged_width}x{merged_height}, å¸ƒå±€: {layout}")
        
        # è·å–FPS
        fps = metadata.get('fps', 30.0)
        
        # åˆ›å»ºè§†é¢‘å†™å…¥å™¨
        video_writer = cv2.VideoWriter(
            str(self.output_video_path), 
            self.fourcc, 
            fps, 
            (merged_width, merged_height)
        )
        
        if not video_writer.isOpened():
            print("âŒ æ— æ³•åˆ›å»ºè§†é¢‘æ–‡ä»¶")
            return False
        
        print(f"ğŸ¥ å¼€å§‹ç”Ÿæˆè§†é¢‘ï¼Œå…±{num_frames}å¸§...")
        
        # ç”Ÿæˆæ¯ä¸€å¸§
        for frame_idx in range(num_frames):
            merged_frame = self._create_merged_frame(
                observation_data, camera_keys, frame_idx, 
                merged_width, merged_height, layout
            )
            
            # æ·»åŠ è½¨è¿¹ä¿¡æ¯overlay
            if trajectory_data:
                merged_frame = self._add_trajectory_overlay(
                    merged_frame, trajectory_data, frame_idx
                )
            
            # æ·»åŠ å¸§ä¿¡æ¯
            merged_frame = self._add_frame_info(
                merged_frame, frame_idx, num_frames
            )
            
            # å†™å…¥è§†é¢‘
            video_writer.write(merged_frame)
            
            # æ˜¾ç¤ºè¿›åº¦
            if (frame_idx + 1) % 10 == 0 or frame_idx == num_frames - 1:
                progress = (frame_idx + 1) / num_frames * 100
                print(f"ğŸ“Š è¿›åº¦: {frame_idx + 1}/{num_frames} ({progress:.1f}%)")
        
        video_writer.release()
        print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {self.output_video_path}")
        return True
    
    def _create_merged_frame(self, observation_data, camera_keys, frame_idx, 
                           merged_width, merged_height, layout):
        """åˆ›å»ºåˆå¹¶å¸§"""
        merged_frame = np.zeros((merged_height, merged_width, 3), dtype=np.uint8)
        
        if layout == "single":
            # å•æ‘„åƒå¤´
            img = observation_data[camera_keys[0]][frame_idx]
            merged_frame = cv2.resize(img, (merged_width, merged_height))
            
        elif layout == "horizontal":
            # æ°´å¹³æ’åˆ—ï¼ˆå·¦å³ï¼‰
            img1 = observation_data[camera_keys[0]][frame_idx]
            img2 = observation_data[camera_keys[1]][frame_idx]
            
            h, w = img1.shape[:2]
            img1_resized = cv2.resize(img1, (w, h))
            img2_resized = cv2.resize(img2, (w, h))
            
            merged_frame[:, :w] = img1_resized
            merged_frame[:, w:] = img2_resized
            
        elif layout == "grid":
            # 2x2ç½‘æ ¼å¸ƒå±€
            h = merged_height // 2
            w = merged_width // 2
            
            for i, camera_key in enumerate(camera_keys[:4]):
                img = observation_data[camera_key][frame_idx]
                img_resized = cv2.resize(img, (w, h))
                
                row = i // 2
                col = i % 2
                y_start = row * h
                y_end = (row + 1) * h
                x_start = col * w
                x_end = (col + 1) * w
                
                merged_frame[y_start:y_end, x_start:x_end] = img_resized
        
        return merged_frame
    
    def _add_trajectory_overlay(self, frame, trajectory_data, frame_idx):
        """æ·»åŠ è½¨è¿¹ä¿¡æ¯overlay"""
        overlay = frame.copy()
        
        # åœ¨å·¦ä¸Šè§’æ˜¾ç¤ºè½¨è¿¹ä¿¡æ¯
        y_pos = 30
        
        if 'position' in trajectory_data and frame_idx < len(trajectory_data['position']):
            pos_data = trajectory_data['position'][frame_idx]
            if len(pos_data) >= 7:
                x, y, z = pos_data[:3]
                rx, ry, rz = pos_data[3:6]
                gripper = pos_data[6]
                
                cv2.putText(overlay, f"Position: ({x:.3f}, {y:.3f}, {z:.3f})", 
                           (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                y_pos += 25
                
                cv2.putText(overlay, f"Rotation: ({rx:.3f}, {ry:.3f}, {rz:.3f})", 
                           (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                y_pos += 25
                
                gripper_status = "OPEN" if gripper < 0.5 else "CLOSE"
                cv2.putText(overlay, f"Gripper: {gripper_status}", 
                           (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)
                y_pos += 25
        
        if 'joint' in trajectory_data and frame_idx < len(trajectory_data['joint']):
            joint_data = trajectory_data['joint'][frame_idx]
            joint_str = ", ".join([f"{j:.2f}" for j in joint_data])
            cv2.putText(overlay, f"Joints: [{joint_str}]", 
                       (10, y_pos), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)
        
        return overlay
    
    def _add_frame_info(self, frame, frame_idx, total_frames):
        """æ·»åŠ å¸§ä¿¡æ¯"""
        # åœ¨å³ä¸Šè§’æ˜¾ç¤ºå¸§ä¿¡æ¯
        frame_text = f"Frame: {frame_idx + 1}/{total_frames}"
        text_size = cv2.getTextSize(frame_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
        x_pos = frame.shape[1] - text_size[0] - 10
        
        cv2.putText(frame, frame_text, (x_pos, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        return frame
    
    def validate_and_create_video(self):
        """éªŒè¯æ•°æ®å¹¶åˆ›å»ºè§†é¢‘"""
        print("ğŸ” å¼€å§‹éªŒè¯HDF5æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        observation_data, trajectory_data, metadata = self.load_hdf5_data()
        
        if observation_data is None:
            return False
        
        # åˆ›å»ºè§†é¢‘
        success = self.create_merged_video(observation_data, trajectory_data, metadata)
        
        if success:
            print(f"\nâœ… éªŒè¯å®Œæˆï¼è§†é¢‘å·²ä¿å­˜åˆ°: {self.output_video_path}")
            print(f"ğŸ“º ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ’­æ”¾è§†é¢‘:")
            print(f"   vlc {self.output_video_path}")
            print(f"   æˆ–")
            print(f"   ffplay {self.output_video_path}")
        
        return success


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯HDF5æ•°æ®å¹¶ç”Ÿæˆåˆå¹¶è§†é¢‘")
    parser.add_argument("hdf5_path", help="HDF5æ•°æ®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("-o", "--output", help="è¾“å‡ºè§†é¢‘è·¯å¾„", default=None)
    parser.add_argument("--fps", type=float, default=30.0, help="è¾“å‡ºè§†é¢‘å¸§ç‡")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.hdf5_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.hdf5_path}")
        return
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = HDF5VideoValidator(args.hdf5_path, args.output)
    validator.fps = args.fps
    
    # æ‰§è¡ŒéªŒè¯
    validator.validate_and_create_video()


if __name__ == "__main__":
    main()
